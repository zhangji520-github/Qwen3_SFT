{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下载环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install peft transformers datasets deepspeed sentencepiece vllm addict\n",
    "\n",
    "pip install flash-attn --no-build-isolation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source /etc/network_turbo\n",
    "\n",
    "git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
    "\n",
    "cd cd LLaMA-Factory/\n",
    "\n",
    "pip install -e \".[torch,metrics]\"   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 启动llamafactory的界面"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "llamafactory-cli webui "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网页点进去报错\n",
    "\n",
    "Visit http://ip:port for Web UI, e.g., http://127.0.0.1:7860\n",
    "* Running on local URL:  http://0.0.0.0:7860\n",
    "\n",
    "To create a public link, set `share=True` in `launch()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解决方案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1 使用隧道工具"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "注意：如果报错\n",
    "\n",
    "importError: vllm>=0.4.3,<=0.8.5 is required for a normal functioning of this module, but found vllm==0.8.5.post1.\n",
    "To fix: run `pip install vllm>=0.4.3,<=0.8.5` or set `DISABLE_VERSION_CHECK=1` to skip this check.\n",
    "\n",
    "\n",
    "单一解决：则在终端输入export DISABLE_VERSION_CHECK=1后再重新开启llama factory ( llamafactory-cli webui )\n",
    "\n",
    "永久解决：echo 'export DISABLE_VERSION_CHECK=1' >> ~/.bashrc 添加环境变量到配置文件（以 bash 为例）：\n",
    "然后source ~/.bashrc\n",
    "\n",
    "虽然 0.8.5.post1 很接近 0.8.5，但 .post1 被视为更高版本，所以不满足 <=0.8.5 的限制。 与其跳过版本检查，也可以尝试修复版本问题：\n",
    "pip install \"vllm>=0.4.3,<=0.8.5\" --force-reinstall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 修改代码下载文件"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "进入 /autodl-tmp/LLaMA-Factory/src/llamafactory/webui\n",
    "\n",
    "打开 interface.py文件\n",
    "\n",
    "将run_web_ui()和run_web_demo()两个函数中最后一行share=gradio_share改为share=True\n",
    "\n",
    "再次运行启动webui (记得是在LLaMA-Factory目录下)\n",
    "\n",
    "终端提示：\n",
    "Could not create share link. Missing file: /root/.cache/huggingface/gradio/frpc/frpc_linux_amd64_v0.3. \n",
    "\n",
    "Please check your internet connection. This can happen if your antivirus software blocks the download of this file. You can install manually by following these steps: \n",
    "\n",
    "\n",
    "1. Download this file: https://cdn-media.huggingface.co/frpc-gradio-0.3/frpc_linux_amd64 记得打开梯子和关闭防火墙\n",
    "2. Rename the downloaded file to: frpc_linux_amd64_v0.3\n",
    "3. Move the file to this location: /root/.cache/huggingface/gradio/frpc\n",
    "\n",
    "mv frpc_linux_amd64 /root/.cache/huggingface/gradio/frpc/frpc_linux_amd64_v0.3\n",
    "cd /root/.cache/huggingface/gradio/frpc/\n",
    "chmod +x frpc_linux_amd64_v0.3 \n",
    "\n",
    "再次运行启动webui (记得是在LLaMA-Factory目录下)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
